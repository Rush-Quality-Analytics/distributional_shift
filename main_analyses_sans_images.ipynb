{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b748901",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from itertools import product\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from numpy import log10\n",
    "import random\n",
    "from math import factorial\n",
    "from scipy.stats import linregress, gaussian_kde, skew\n",
    "from scipy import stats\n",
    "from scipy.spatial import distance\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from statsmodels.stats.outliers_influence import summary_table\n",
    "import statsmodels.api as sm\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "%config InlineBackend.print_figure_kwargs={'facecolor' : \"w\"}\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abecec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cum_dists(n_obs, n_bins, cum=True):\n",
    "    \"\"\"Generate all possible discrete distributions for a given number of \n",
    "        observations distributed across a given number of bins\n",
    "    \"\"\"\n",
    "    \n",
    "    def partitions(n, k):\n",
    "        if k == 1:\n",
    "            yield (n,)\n",
    "        else:\n",
    "            for i in range(n + 1):\n",
    "                for result in partitions(n - i, k - 1):\n",
    "                    yield (i,) + result\n",
    "\n",
    "    for combo in partitions(n_obs, n_bins):\n",
    "        if cum:\n",
    "            cum_dist = [sum(combo[:i + 1]) for i in range(n_bins)]\n",
    "            yield cum_dist\n",
    "        else:\n",
    "            yield combo\n",
    "\n",
    "            \n",
    "def count_pts_within_radius(x, y, radius, scale=0):\n",
    "    \"\"\"Count the number of points within a fixed radius in 2D space\"\"\"\n",
    "    \n",
    "    raw_data = np.array([x, y])\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    raw_data = raw_data.transpose()\n",
    "    \n",
    "    # Get unique data points by adding each pair of points to a set\n",
    "    unique_points = set()\n",
    "    for xval, yval in raw_data:\n",
    "        unique_points.add((xval, yval))\n",
    "    \n",
    "    count_data = []\n",
    "    for a, b in unique_points:\n",
    "        if scale == 'sqrt':\n",
    "            num_neighbors = len(x[((sqrt(x) - sqrt(a)) ** 2 +\n",
    "                                   (sqrt(y) - sqrt(b)) ** 2) <= sqrt(radius) ** 2])\n",
    "        else:        \n",
    "            num_neighbors = len(x[((x - a) ** 2 + (y - b) ** 2) <= radius ** 2])\n",
    "        count_data.append((a, b, num_neighbors))\n",
    "    return count_data\n",
    "\n",
    "\n",
    "\n",
    "def plot_color_by_pt_dens(x, y, radius, loglog=0, scale=0, plot_obj=None, point_size=10):\n",
    "    \n",
    "    \"\"\"Plot bivariate relationships with large n using color for point density\n",
    "\n",
    "    Inputs:\n",
    "    x & y -- variables to be plotted\n",
    "    radius -- the linear distance within which to count points as neighbors\n",
    "    scale -- a flag to indicate the use of a scale plot (scale = 1)\n",
    "\n",
    "    The color of each point in the plot is determined by the logarithm (base 10)\n",
    "    of the number of points that occur with a given radius of the focal point,\n",
    "    with hotter colors indicating more points. The number of neighboring points\n",
    "    is determined in linear space regardless of whether a scale plot is\n",
    "    presented.\n",
    "    \"\"\"\n",
    "    \n",
    "    plot_data = count_pts_within_radius(x, y, radius, scale)\n",
    "    sorted_plot_data = np.array(sorted(plot_data, key=lambda point: point[2]))\n",
    "\n",
    "    if plot_obj == None:\n",
    "        plot_obj = plt.axes()\n",
    "        \n",
    "    plot_obj.scatter(sorted_plot_data[:, 0],\n",
    "            sorted_plot_data[:, 1],\n",
    "            facecolors='none',\n",
    "            s = point_size, \n",
    "            edgecolors='0.1', \n",
    "            linewidths=1., \n",
    "            #cmap='Greys'\n",
    "            )\n",
    "    \n",
    "    # plot points\n",
    "    c = np.array(sorted_plot_data[:, 2])**0.25\n",
    "    c = np.max(c) - c\n",
    "    plot_obj.scatter(sorted_plot_data[:, 0],\n",
    "                    sorted_plot_data[:, 1],\n",
    "                    c = c,\n",
    "                    s = point_size, \n",
    "                    edgecolors='k', \n",
    "                    linewidths=0.0, \n",
    "                    cmap='Greys_r',\n",
    "                    #alpha = 0.5,\n",
    "                    )\n",
    "        \n",
    "    return plot_obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b16089",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# 2. Calculating distributional shift (DS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d21a5af",
   "metadata": {},
   "source": [
    "We define distributional shift (DS) as the concentration of frequencies towards the lowest discrete class, which we measure via the sum of cumulative frequencies.\n",
    "\n",
    "We begin by simply deriving DS as: \n",
    "\n",
    "$$DS = (\\sum{F}/n - 1)/(k-1)$$<br><br>\n",
    "\n",
    "We then refine DS to include exponentiated cumulative frequencies:\n",
    "\n",
    "$$DS = (\\sum{F^{z}}/n^{z} - 1)/(k-1)$$<br><br>\n",
    "\n",
    "Finally, we refine the exponent (*z*) to take fractional values:\n",
    "\n",
    "$$DS = (\\sum{F^{k+1/k}}/n^{k+1/k} - 1)/(k-1)$$<br><br>\n",
    "\n",
    "Below, we provide a function to allow users to explore the calculation of DS and to vary the number of observations (n_obs) and the number of bins (n_bins)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4b0bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "k = 4\n",
    "\n",
    "# Generate cumulative distributions one at a time and add to a list\n",
    "_set = []\n",
    "for dist in generate_cum_dists(n, k, cum=False):\n",
    "    _set.append(dist)\n",
    "\n",
    "\n",
    "DS_ls = []\n",
    "for i, d in enumerate(_set):\n",
    "\n",
    "    # get cumulative distribution\n",
    "    cd = np.array([sum(d[:i+1]) for i in range(len(d))])\n",
    "    \n",
    "    # exponentiate cumulative frequencies\n",
    "    z = 1\n",
    "    z = (k + 1)/k\n",
    "    \n",
    "    DS = str(((sum(cd**z)/(n**z)) - 1) / (k - 1))\n",
    "    DS = DS[:8]\n",
    "    \n",
    "    if sum(cd)/(n) == 2.2:    \n",
    "        #print(i+1, d, tuple(cd), sum(cd**z)/(n**z))\n",
    "        print(i+1, d, tuple(cd), DS)\n",
    "\n",
    "    DS_ls.append(DS)\n",
    "    \n",
    "print('\\n')\n",
    "print(len(DS_ls), 'members of the feasible set for n and k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17789220",
   "metadata": {},
   "source": [
    "# 3. Generate manuscript figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385170d4",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996bffc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = 10\n",
    "n_bins = 5\n",
    "fig = plt.figure(figsize=(10.5, 7))\n",
    "   \n",
    "    \n",
    "################  TOP ROW  ###########\n",
    "\n",
    "_set = []\n",
    "for dist in generate_cum_dists(n_obs, n_bins, cum=False):\n",
    "    _set.append(dist)\n",
    "\n",
    "plot_num = 1\n",
    "for z in [2, 4, 7]:\n",
    "    \n",
    "    t1 = []\n",
    "    t2 = []\n",
    "    for d in _set:\n",
    "\n",
    "        cd = [sum(d[:i+1]) for i in range(len(d))]\n",
    "        ncd = np.array(cd)/max(cd)\n",
    "        t1.append(sum(ncd))\n",
    "\n",
    "        G = [sum(d[:i+1])**(z) for i in range(len(d))]\n",
    "        G_ = np.array(G)/(n_obs**z)\n",
    "\n",
    "        t2.append(sum(G_))\n",
    "\n",
    "    print('n:', n_obs, 'k:', n_bins)\n",
    "    print('cardinality:', len(t1), '| no. of unique ΣF/n:', \n",
    "          len(list(set(t1))), '| no. of unique ΣF^z/n^z:', len(list(set(t2))), '\\n')\n",
    "\n",
    "    ax = plt.subplot(2, 3, plot_num)\n",
    "    plt.scatter(t1, t2, s=1, c='k')\n",
    "    plt.xlabel(r'$\\sum{F/n}$', fontsize= 14)\n",
    "    plt.ylabel(r'$\\sum{F^{' + str(z) + '}/n^{' + str(z) + '}}$', fontsize= 14)\n",
    "    \n",
    "    s = '|A' + r'$_{n' + '=' + str(n_obs) + ', ' + 'k' + '=' + str(n_bins) + '}$' + '| = ' + str(len(_set)) + '\\n'\n",
    "    s += '\\nValues of ' + r'$\\sum{F^{' + str(z) + '}/n^{' + str(z) + '}}$' + '\\n = ' + str(len(list(set(t2))))\n",
    "    \n",
    "    plt.text(1.01, 3.65, s, fontsize=10)\n",
    "    plt.tick_params(axis='both', labelsize=10)\n",
    "    plot_num += 1\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "################  BOTTOM ROW  ###########  \n",
    "\n",
    "N_obs = [10, 10, 20]\n",
    "N_bins = [5, 10, 10]\n",
    "sets = []\n",
    "for i, n_obs in enumerate(N_obs):\n",
    "    n_bins = N_bins[i]\n",
    "    \n",
    "    _set = []\n",
    "    for dist in generate_cum_dists(n_obs, n_bins, cum=False):\n",
    "        _set.append(dist)\n",
    "    sets.append(_set)\n",
    "\n",
    "\n",
    "\n",
    "for i, n_obs in enumerate(N_obs):\n",
    "    n_bins = N_bins[i]\n",
    "    \n",
    "    _set = sets[i]\n",
    "    \n",
    "    t1 = []\n",
    "    t2 = []\n",
    "    for d in _set:\n",
    "\n",
    "        cd = [sum(d[:ii+1]) for ii in range(len(d))]\n",
    "        ncd = np.array(cd)/max(cd)\n",
    "        t1.append(sum(ncd))\n",
    "\n",
    "        z = (n_bins + 1)/n_bins\n",
    "        G = [sum(d[:ii+1])**(z) for ii in range(len(d))]\n",
    "        G_ = np.array(G)/(n_obs**z)\n",
    "        t2.append(sum(G_))\n",
    "\n",
    "    print('n:', n_obs, 'k:', n_bins)\n",
    "    print('cardinality:', len(t1), '| no. of unique ΣF/n:', \n",
    "          len(list(set(t1))), '| no. of unique ΣF^z/n^z:', len(list(set(t2))), '\\n')\n",
    "    \n",
    "    s = '|A' + r'$_{n' + '=' + str(n_obs) + ', ' + 'k' + '=' + str(n_bins) + '}$' + '| = ' + str(len(_set)) + '\\n'\n",
    "    s += '\\nValues of ' + r'$\\sum{F_{i}^{' + str(z) + '}/n^{' + str(z) + '}}$' + '\\n = ' + str(len(_set))\n",
    "    \n",
    "    if len(_set) > 10**5: \n",
    "        indices = np.random.choice(len(_set), 10**5, replace=False)\n",
    "        t1 = np.array(t1)\n",
    "        t2 = np.array(t2)\n",
    "        t1 = t1[indices]\n",
    "        t2 = t2[indices]\n",
    "        t1 = t1.tolist()\n",
    "        t2 = t2.tolist()\n",
    "    \n",
    "    ax = plt.subplot(2, 3, plot_num)\n",
    "    plt.scatter(t1, t2, s=1, c='k')\n",
    "    plt.xlabel(r'$\\sum{F/n}$', fontsize= 14)\n",
    "    plt.ylabel(r'$\\sum{F^{' + str(z) + '}/n^{' + str(z) + '}}$', fontsize= 14)\n",
    "    \n",
    "    if plot_num == 4:\n",
    "        plt.text(1. * min(t1), 0.73*max(t2), s, fontsize=10)\n",
    "    elif plot_num == 5:\n",
    "        plt.text(1. * min(t1), 0.7*max(t2), s, fontsize=10)\n",
    "    elif plot_num == 6:\n",
    "        plt.text(0.9 * min(t1), 0.71*max(t2), s, fontsize=10)\n",
    "        \n",
    "    plt.tick_params(axis='both', labelsize=10)\n",
    "    plot_num += 1\n",
    "    \n",
    "\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.subplots_adjust(hspace=0.45, wspace=0.4)\n",
    "plt.savefig('Fig1.jpg', bbox_inches='tight', format='jpg', dpi=600)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50180e0",
   "metadata": {},
   "source": [
    "----\n",
    "# Comparing RS to established measures\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13e90fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def histogram_intersection(p, q):\n",
    "    \n",
    "    # Calculate histogram intersection\n",
    "    \n",
    "    # q is the reference distribution\n",
    "    # p is the query distribution\n",
    "    \n",
    "    minima = np.minimum(p, q)\n",
    "    hi = np.true_divide(np.sum(minima), np.sum(p))\n",
    "    \n",
    "    if hi > 1 or hi < 0:\n",
    "        print('Error, HI =', hi)\n",
    "        print(p)\n",
    "        print(q)\n",
    "        return\n",
    "    return hi\n",
    "\n",
    "\n",
    "def chi_square_distance(p, q):\n",
    "    \n",
    "    # Calculate chi-square distance\n",
    "    \n",
    "    # q is the reference distribution\n",
    "    # p is the query distribution\n",
    "    \n",
    "    p = np.array(p)/np.sum(p)\n",
    "    q = np.array(q)/np.sum(q)\n",
    "\n",
    "    return np.sum(((p - q)**2 / (p + q))) / 2\n",
    "\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    \n",
    "    # Calculate Kullback-Leibler Divergence\n",
    "    \n",
    "    # q is the reference distribution\n",
    "    # p is the query distribution\n",
    "    \n",
    "    # Ensure both lists are numpy arrays with dtype=float\n",
    "    \n",
    "    p = np.array(p, dtype=float)\n",
    "    q = np.array(q, dtype=float)\n",
    "    \n",
    "    p /= p.sum()\n",
    "    q /= q.sum()\n",
    "    \n",
    "    kl_div = np.sum(p * np.log(p / q))\n",
    "    return kl_div\n",
    "\n",
    "\n",
    "def earth_movers_distance(p, q):\n",
    "    \n",
    "    # Calculate Earth Mover's Distance\n",
    "    \n",
    "    # q is the reference distribution\n",
    "    # p is the query distribution\n",
    "    \n",
    "    # Ensure both lists are numpy arrays with dtype=float\n",
    "    p = np.array(p, dtype=float)\n",
    "    q = np.array(q, dtype=float)\n",
    "\n",
    "    # Normalize the distributions to ensure they sum to 1\n",
    "    p /= p.sum()\n",
    "    q /= q.sum()\n",
    "\n",
    "    # Calculate cumulative distributions\n",
    "    P = np.cumsum(p)\n",
    "    Q = np.cumsum(q)\n",
    "\n",
    "    # Calculate the cost matrix\n",
    "    C = np.abs(np.subtract.outer(P, Q))\n",
    "\n",
    "    # Solve the linear sum assignment problem\n",
    "    row_ind, col_ind = linear_sum_assignment(C)\n",
    "\n",
    "    # Calculate the Earth Mover's Distance\n",
    "    emd = C[row_ind, col_ind].sum()\n",
    "\n",
    "    return emd\n",
    "\n",
    "\n",
    "def kolmogorov_smirnov_distance(p, q):\n",
    "    \n",
    "    # Calculate KS distance\n",
    "    \n",
    "    # q is the reference distribution\n",
    "    # p is the query distribution\n",
    "    \n",
    "    # Ensure both lists are numpy arrays with dtype=float\n",
    "    p = np.array(p, dtype=float)\n",
    "    q = np.array(q, dtype=float)\n",
    "\n",
    "    # Normalize the distributions to ensure they sum to 1\n",
    "    p /= p.sum()\n",
    "    q /= q.sum()\n",
    "\n",
    "    # Calculate cumulative distributions\n",
    "    P = np.cumsum(p)\n",
    "    Q = np.cumsum(q)\n",
    "\n",
    "    # Calculate the KS distance\n",
    "    ks_distance = np.max(np.abs(P - Q))\n",
    "\n",
    "    return ks_distance\n",
    "\n",
    "\n",
    "def rank_probability_score(p, q):\n",
    "    \n",
    "    # Calculate the ranked probability score\n",
    "    \n",
    "    # q is the reference distribution\n",
    "    # p is the query distribution\n",
    "    \n",
    "    # Ensure both lists are numpy arrays with dtype=float\n",
    "    p = np.array(p, dtype=float)\n",
    "    q = np.array(q, dtype=float)\n",
    "\n",
    "    # Normalize the distributions to ensure they sum to 1\n",
    "    p /= p.sum()\n",
    "    q /= q.sum()\n",
    "\n",
    "    # Calculate cumulative distributions\n",
    "    P = np.cumsum(p)\n",
    "    Q = np.cumsum(q)\n",
    "\n",
    "    # Calculate Rank Probability Score\n",
    "    rps = np.sum((P - Q)**2)\n",
    "\n",
    "    return rps\n",
    "\n",
    "\n",
    "def RDS(p, q):\n",
    "    \n",
    "    # Calculate Relative distribution shift\n",
    "    \n",
    "    # q is the reference distribution\n",
    "    # p is the query distribution\n",
    "    p_bins = len(p)\n",
    "    p_obs = sum(p)\n",
    "    \n",
    "    q_bins = len(q)\n",
    "    q_obs = sum(q)\n",
    "    \n",
    "    z_p = (p_bins + 1)/p_bins\n",
    "    p = [sum(p[:ii+1])**(z_p) for ii in range(len(p))]\n",
    "    Sp = np.sum(np.array(p)/(p_obs**z_p)) - 1\n",
    "    Sp = Sp/(p_bins - 1)\n",
    "    \n",
    "    z_q = (q_bins + 1)/q_bins\n",
    "    q = [sum(q[:ii+1])**(z_q) for ii in range(len(q))]\n",
    "    Sq = np.sum(np.array(q)/(q_obs**z_q)) - 1\n",
    "    Sq = Sq/(q_bins - 1)\n",
    "    \n",
    "    return Sq - Sp\n",
    "    \n",
    "    \n",
    "def DS(p):\n",
    "    p_bins = len(p)\n",
    "    p_obs = sum(p)\n",
    "    \n",
    "    z_p = (p_bins + 1)/p_bins\n",
    "    p = [sum(p[:ii+1])**(z_p) for ii in range(len(p))]\n",
    "    Sp = np.sum(np.array(p)/(p_obs**z_p)) - 1\n",
    "    ds = Sp/(p_bins - 1)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed21320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = 100\n",
    "n_bins = 5\n",
    "# Generate cumulative distributions one at a time and add to a list\n",
    "feasible_set = []\n",
    "for dist in generate_cum_dists(n_obs, n_bins, cum=False):\n",
    "    feasible_set.append(dist)\n",
    "\n",
    "num = f\"{len(feasible_set):,}\"\n",
    "print('There are', num, 'possible discrete frequency distributions for', n_obs, 'observations distributed among', n_bins, 'bins, when allowing bins to have values of 0.\\n')\n",
    "\n",
    "ls = []\n",
    "for d in feasible_set:\n",
    "    \n",
    "    z = (n_bins + 1)/n_bins\n",
    "    G = [sum(d[:ii+1])**(z) for ii in range(len(d))]\n",
    "    G_ = np.array(G)/(n_obs**z)\n",
    "    ls.append( (sum(G_) - 1) / (n_bins - 1) )\n",
    "    \n",
    "num = f\"{len(list(set(ls))):,}\"\n",
    "print('There are ' + num + ' unique values of shift.\\n')\n",
    "\n",
    "print('min shift =', min(ls))\n",
    "print('max shift =', max(ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60044cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RDS_vals = []\n",
    "csd = []\n",
    "ks_dists = []\n",
    "hist_ints = []\n",
    "kl_divs = []\n",
    "em_dists = []\n",
    "rp_scores = []\n",
    "d1_ls = []\n",
    "d2_ls = []\n",
    "dif_med = []\n",
    "dif_var = []\n",
    "dif_skew = []\n",
    "\n",
    "num = 0\n",
    "start_time_0 = time.time()\n",
    "while num < 10**5:\n",
    "    \n",
    "    d1, d2 = random.sample(feasible_set, 2)\n",
    "    \n",
    "    d1_ls.append(d1)\n",
    "    d2_ls.append(d2)\n",
    "    \n",
    "    # Relative distributional shift\n",
    "    j = RDS(d1, d2)\n",
    "    RDS_vals.append(j)\n",
    "    \n",
    "    # KS distance\n",
    "    j = kolmogorov_smirnov_distance(d1, d2)\n",
    "    ks_dists.append(j)\n",
    "        \n",
    "    # Histogram intersection\n",
    "    j = histogram_intersection(d1, d2)\n",
    "    hist_ints.append(1-j)\n",
    "    \n",
    "    # chi-square distance\n",
    "    j = chi_square_distance(d1, d2)\n",
    "    csd.append(j)\n",
    "    \n",
    "    # KL divergence\n",
    "    j = kl_divergence(d1, d2)\n",
    "    kl_divs.append(j)\n",
    "    \n",
    "    # Rank probability score\n",
    "    j = rank_probability_score(d1, d2)\n",
    "    rp_scores.append(j)\n",
    "    \n",
    "    # Earth movers distance\n",
    "    j = earth_movers_distance(d1, d2)\n",
    "    em_dists.append(j)\n",
    "    \n",
    "    # difference in median\n",
    "    j = np.nanmedian(d1) - np.nanmedian(d2)\n",
    "    dif_med.append(j)\n",
    "    \n",
    "    # difference in variance\n",
    "    v1 = []\n",
    "    for i, f in enumerate(d1): v1.extend([i+1]*f)\n",
    "        \n",
    "    v2 = []\n",
    "    for i, f in enumerate(d2): v2.extend([i+1]*f)\n",
    "    \n",
    "    dif_var.append(np.var(v1) - np.var(v2))\n",
    "    dif_skew.append(skew(v1) - skew(v2))\n",
    "    \n",
    "    num += 1\n",
    "\n",
    "end_time = time.time()\n",
    "print('Completed in {:.3f} seconds'.format(end_time - start_time_0))\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f9e278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "X_lists = [np.sqrt(csd).tolist(), \n",
    "           ks_dists, \n",
    "           hist_ints, \n",
    "           np.sqrt(kl_divs).tolist(), \n",
    "           em_dists, \n",
    "           np.sqrt(rp_scores).tolist(),\n",
    "           np.abs(RDS_vals),\n",
    "          ]\n",
    "\n",
    "x_labs = [r\"$\\sqrt{CSD}$\",\n",
    "          'KSD',\n",
    "          '1-HI',\n",
    "          r\"$\\sqrt{KLD}$\",\n",
    "          'EMD',\n",
    "          r\"$\\sqrt{RPS}$\",\n",
    "          '|RDS|',\n",
    "         ]\n",
    "\n",
    "fig = plt.figure(figsize=(13, 13))\n",
    "\n",
    "ind = 1\n",
    "for i, y_ls in enumerate(X_lists):\n",
    "    xlab = x_labs[i]\n",
    "    for j, x_ls in enumerate(X_lists):\n",
    "        ylab = x_labs[j]\n",
    "        \n",
    "        if ind not in [8, 15, 16, 22, 23, 24, 29, 30, 31, 32, 36, 37, 38, 39, 40,\n",
    "                      43, 44, 45, 46, 47, 48]:\n",
    "            ind += 1\n",
    "            continue\n",
    "\n",
    "        x = []\n",
    "        y = []\n",
    "        for i, xval in enumerate(x_ls):\n",
    "            if np.isnan(xval) == False and np.isinf(xval) == False:\n",
    "                yval = y_ls[i]\n",
    "                if np.isnan(yval) == False and np.isinf(yval) == False:\n",
    "                    x.append(xval)\n",
    "                    y.append(yval)\n",
    "            \n",
    "        plot_color_by_pt_dens(x, y, radius=0.05, loglog=0, plot_obj=plt.subplot(7, 7, ind), point_size=10)\n",
    "\n",
    "        slope, intercept, r_val, p_val, std_err = linregress(x, y)\n",
    "        fitted_vals = slope * np.array(x) + intercept\n",
    "        \n",
    "        s = r'$r^{2}$' + ' = ' + str(round(r_val**2, 2))\n",
    "        plt.plot(x, fitted_vals, color='k', linewidth=1, label=s)\n",
    "\n",
    "        plt.tick_params(axis='both', left=False, top=False, right=False, bottom=False, \n",
    "                        labelleft=False, labeltop=False, \n",
    "                        labelright=False, labelbottom=False,\n",
    "                       )\n",
    "        \n",
    "        legend = plt.legend(loc='lower center', bbox_to_anchor=(0.1, 0.96), borderaxespad=0., frameon=False)\n",
    "        legend.set_alpha(0)\n",
    "\n",
    "        # Remove the line representing data\n",
    "        for line in legend.get_lines():\n",
    "            line.set_linestyle('None')\n",
    "            line.set_marker(None)\n",
    "\n",
    "        plt.xlabel(xlab, fontsize = 10)\n",
    "        plt.ylabel(ylab, fontsize = 10)\n",
    "        ind += 1\n",
    "    \n",
    "\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "plt.savefig('Fig3.jpg', bbox_inches='tight', format='jpg', dpi=600)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd73dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_lists = [ks_dists, \n",
    "           em_dists, \n",
    "           np.sqrt(rp_scores).tolist(),\n",
    "           np.sqrt(csd).tolist(), \n",
    "           hist_ints, \n",
    "           np.sqrt(kl_divs).tolist(),\n",
    "          ]\n",
    "\n",
    "x_labs = ['Kolmogorov-Smirnov distance',\n",
    "          'Earth Mover Distance',\n",
    "          r\"$\\sqrt{RPS}$\",\n",
    "          r\"$\\sqrt{CSD}$\",\n",
    "          '1 - Histogram Intersection',\n",
    "          r\"$\\sqrt{KLD}$\",\n",
    "          ]\n",
    "\n",
    "fig = plt.figure(figsize=(11, 11))\n",
    "\n",
    "text_x_vals = [0.05, 0.15, 0.05, \n",
    "               0.05, 0.05, 0.1]\n",
    "\n",
    "for i, x_ls in enumerate(X_lists):\n",
    "    \n",
    "    xv = []\n",
    "    yv = []\n",
    "    ct = 0\n",
    "    for ii, val in enumerate(x_ls):\n",
    "        \n",
    "        if val > 0 and val < 10**10:\n",
    "            xv.append(val)\n",
    "            yv.append(RDS_vals[ii])\n",
    "\n",
    "    plot_color_by_pt_dens(xv, yv, radius=0.05, loglog=0, plot_obj=plt.subplot(3, 3, i+1), point_size=10)\n",
    "    slope, intercept, r_val, p_val, std_err = linregress(xv, np.abs(yv))\n",
    "    fitted_vals = slope * np.array(x) + intercept\n",
    "    s = r'$r^{2}$' + ' = ' + str(round(r_val**2, 2))\n",
    "    \n",
    "    plt.text(text_x_vals[i], 0.7, s, fontsize=12)\n",
    "    plt.xlabel(x_labs[i], fontsize= 12)\n",
    "    plt.ylabel('RDS', fontsize= 12)\n",
    "    plt.tick_params(axis='both', labelsize=10)\n",
    "    \n",
    "fig.patch.set_facecolor('white')\n",
    "plt.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "plt.savefig('Fig4.jpg', bbox_inches='tight', format='jpg', dpi=600)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = [21, 2, 0, 2, 21]\n",
    "f2 = [1, 1, 42, 1, 1]\n",
    "print('f1:', f1)\n",
    "print('f2:', f2, '\\n')\n",
    "\n",
    "print('DS(f1):', round(DS(f1),3))\n",
    "print('DS(f2):', round(DS(f2),3), '\\n')\n",
    "print('RDS = ', round(RDS(f1, f2),3))\n",
    "\n",
    "#print('Rank probability score = ', round(rank_probability_score(f1, f2)/rank_probability_score(mf1, mf2), 3))\n",
    "print('1 - Histogram Intersection = ', round(1 - histogram_intersection(f1, f2),3))\n",
    "d = chi_square_distance(f1, f2)\n",
    "print('Chi-square distance = ', d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb13ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for the Poisson\n",
    "lambda_parameter = 5  # Adjust this parameter as needed\n",
    "\n",
    "dists = []\n",
    "\n",
    "while len(dists) < 10**5:\n",
    "    \n",
    "    data = np.random.poisson(lambda_parameter, 100)\n",
    "    hist_vals, bins = np.histogram(data, bins=5, density=False)\n",
    "    hist_vals = hist_vals.tolist()\n",
    "    dists.append(hist_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8775fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDS_vals = []\n",
    "csd = []\n",
    "ks_dists = []\n",
    "hist_ints = []\n",
    "kl_divs = []\n",
    "em_dists = []\n",
    "rp_scores = []\n",
    "d1_ls = []\n",
    "d2_ls = []\n",
    "\n",
    "num = 0\n",
    "start_time_0 = time.time()\n",
    "while num < 10**5:\n",
    "    \n",
    "    d1, d2 = random.sample(dists, 2)\n",
    "    \n",
    "    d1_ls.append(d1)\n",
    "    d2_ls.append(d2)\n",
    "    \n",
    "    # Relative distributional shift\n",
    "    j = RDS(d1, d2)\n",
    "    RDS_vals.append(j)\n",
    "    \n",
    "    # KS distance\n",
    "    j = kolmogorov_smirnov_distance(d1, d2)\n",
    "    ks_dists.append(j)\n",
    "        \n",
    "    # Histogram intersection\n",
    "    j = histogram_intersection(d1, d2)\n",
    "    hist_ints.append(1-j)\n",
    "    \n",
    "    # chi-square distance\n",
    "    j = chi_square_distance(d1, d2)\n",
    "    csd.append(j)\n",
    "    \n",
    "    # KL divergence\n",
    "    j = kl_divergence(d1, d2)\n",
    "    kl_divs.append(j)\n",
    "\n",
    "    \n",
    "    # Rank probability score\n",
    "    j = rank_probability_score(d1, d2)\n",
    "    rp_scores.append(j)\n",
    "    \n",
    "    # Earth movers distance\n",
    "    j = earth_movers_distance(d1, d2)\n",
    "    em_dists.append(j)\n",
    "    \n",
    "    num += 1\n",
    "\n",
    "end_time = time.time()\n",
    "print('Completed in {:.3f} seconds'.format(end_time - start_time_0), '\\n')\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d1ce05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "lists = [np.sqrt(csd).tolist(),\n",
    "         ks_dists, \n",
    "         hist_ints,\n",
    "         np.sqrt(kl_divs).tolist(), \n",
    "         em_dists, \n",
    "         np.sqrt(rp_scores).tolist(),\n",
    "         np.abs(RDS_vals),\n",
    "        ]\n",
    "\n",
    "labs = [r\"$\\sqrt{CSD}$\",\n",
    "          'KSD',\n",
    "          '1-HI',\n",
    "          r\"$\\sqrt{KLD}$\",\n",
    "          'EMD',\n",
    "          r\"$\\sqrt{RPS}$\",\n",
    "          '|RDS|',\n",
    "         ]\n",
    "\n",
    "fig = plt.figure(figsize=(13, 13))\n",
    "\n",
    "ind = 1\n",
    "for i, y_ls in enumerate(lists):\n",
    "    xlab = labs[i]\n",
    "    for j, x_ls in enumerate(lists):\n",
    "        ylab = labs[j]\n",
    "        \n",
    "        if ind not in [8, 15, 16, 22, 23, 24, 29, 30, 31, 32, 36, 37, 38, 39, 40,\n",
    "                      43, 44, 45, 46, 47, 48]:\n",
    "            ind += 1\n",
    "            continue\n",
    "\n",
    "        x = []\n",
    "        y = []\n",
    "        for i, xval in enumerate(x_ls):\n",
    "            if np.isnan(xval) == False and np.isinf(xval) == False:\n",
    "                yval = y_ls[i]\n",
    "                if np.isnan(yval) == False and np.isinf(yval) == False:\n",
    "                    x.append(xval)\n",
    "                    y.append(yval)\n",
    "            \n",
    "        plot_color_by_pt_dens(x, y, radius=0.05, loglog=0, plot_obj=plt.subplot(7, 7, ind), point_size=10)\n",
    "\n",
    "        slope, intercept, r_val, p_val, std_err = linregress(x, y)\n",
    "        fitted_vals = slope * np.array(x) + intercept\n",
    "        \n",
    "        s = r'$r^{2}$' + ' = ' + str(round(r_val**2, 2))\n",
    "        plt.plot(x, fitted_vals, color='k', linewidth=1, label=s)\n",
    "\n",
    "        plt.tick_params(axis='both', left=False, top=False, right=False, bottom=False, \n",
    "                        labelleft=False, labeltop=False, \n",
    "                        labelright=False, labelbottom=False,\n",
    "                       )\n",
    "        \n",
    "        legend = plt.legend(loc='lower center', bbox_to_anchor=(0.1, 0.96), borderaxespad=0., frameon=False)\n",
    "        legend.set_alpha(0)\n",
    "\n",
    "        for line in legend.get_lines():\n",
    "            line.set_linestyle('None') \n",
    "            line.set_marker(None)\n",
    "\n",
    "        plt.xlabel(xlab, fontsize = 10)\n",
    "        plt.ylabel(ylab, fontsize = 10)\n",
    "        ind += 1\n",
    "    \n",
    "\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "plt.savefig('Fig5.jpg', bbox_inches='tight', format='jpg', dpi=600)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc2097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_lists = [ks_dists, \n",
    "           em_dists, \n",
    "           np.sqrt(rp_scores).tolist(),\n",
    "           np.sqrt(csd).tolist(), \n",
    "           hist_ints, \n",
    "           np.sqrt(kl_divs).tolist(),\n",
    "          ]\n",
    "\n",
    "x_labs = ['Kolmogorov-Smirnov distance',\n",
    "          'Earth Mover Distance',\n",
    "          r\"$\\sqrt{RPS}$\",\n",
    "          r\"$\\sqrt{CSD}$\",\n",
    "          '1 - Histogram Intersection',\n",
    "          r\"$\\sqrt{KLD}$\",\n",
    "          ]\n",
    "\n",
    "fig = plt.figure(figsize=(11, 11))\n",
    "\n",
    "text_x_vals = [0.02, 0.05, 0.02, \n",
    "               0.02, 0.02, 0.05]\n",
    "\n",
    "for i, x_ls in enumerate(X_lists):\n",
    "    \n",
    "    xv = []\n",
    "    yv = []\n",
    "    ct = 0\n",
    "    for ii, val in enumerate(x_ls):\n",
    "        \n",
    "        if val > 0 and val < 10**10:\n",
    "            xv.append(val)\n",
    "            yv.append(RDS_vals[ii])\n",
    "\n",
    "    plot_color_by_pt_dens(xv, yv, radius=0.05, loglog=0, plot_obj=plt.subplot(3, 3, i+1), point_size=10)\n",
    "    slope, intercept, r_val, p_val, std_err = linregress(xv, np.abs(yv))\n",
    "    fitted_vals = slope * np.array(x) + intercept\n",
    "    s = r'$r^{2}$' + ' = ' + str(round(r_val**2, 2))\n",
    "    \n",
    "    plt.text(text_x_vals[i], 0.35, s, fontsize=12)\n",
    "    plt.xlabel(x_labs[i], fontsize= 12)\n",
    "    plt.ylabel('RDS', fontsize= 12)\n",
    "    plt.tick_params(axis='both', labelsize=10)\n",
    "    \n",
    "fig.patch.set_facecolor('white')\n",
    "plt.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "plt.savefig('Fig6.jpg', bbox_inches='tight', format='jpg', dpi=600)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e696cc30",
   "metadata": {},
   "source": [
    "# Ecological analysis\n",
    "\n",
    "## Analyzing species rarity\n",
    " \n",
    "The analysis of shift is highly relevant to the study of species-abundance distributions (SADs) (Fig 8a) [10, 12, 14, 21]. These histograms of species abundance underpin thousands of ecological studies spanning all domains of life and major habitats [10, 12, 14, 21]. SADs are often compared to theoretical predictions, to SADs sampled from other communities of similar taxa, and are the basis for many measures of species dominance, diversity, evenness, and rarity [9, 10, 12, 14, 21]. Consequently, the field of ecology is replete with techniques for analyzing SADs and for quantifying the aspects of biodiversity they contain.\n",
    "\n",
    "\n",
    "In considering that RDS provides a means of quantifying the magnitude and direction by which one SAD is concentrated to lesser or greater abundances relative to another SAD,  \n",
    "\n",
    "Beyond providing yet another means of comparing SADs, DS and RDS provide an intuitive and easily interpretable means of quantifying species rarity, i.e., the concentration of species at low abundances [14, 21]. \n",
    "\n",
    "However, \n",
    "\n",
    "As the concentration of frequencies away from the discrete class having the greatest value, DS is highly similar in concept to species rarity. Consequently, given its bounded values and intuitive interpretation, we asked whether DS provides a preferrable measure of species rarity. in particular, because skewness based measures are not bounded an\n",
    "\n",
    "Unlike skewness-based measures, RDS is bounded, intuitive, and directly reflects the concentration of species towards the class of lowest abundance. Likewise, RDS represents a better comparative metric of species rarity for the same reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deb13da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rlogskew(sad):\n",
    "    '''\n",
    "    Calculation of rarity used in:\n",
    "        A.E. Magurran and B.J. McGill, eds. Biological diversity: frontiers in measurement and assessment. \n",
    "        OUP Oxford, 2010.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    S = len(sad)\n",
    "\n",
    "    if S <= 2.0:\n",
    "        print('S < 2, cannot compute log-skew')\n",
    "        sys.exit()\n",
    "\n",
    "    sad = np.log10(sad)\n",
    "    mu = np.mean(sad)\n",
    "\n",
    "    num = 0\n",
    "    denom = 0\n",
    "    for ni in sad:\n",
    "        num += ((ni - mu)**3.0)/S\n",
    "        denom += ((ni - mu)**2.0)/S\n",
    "\n",
    "    t1 = num/(denom**(3.0/2.0))\n",
    "    t2 = (S/(S - 2.0)) * np.sqrt((S - 1.0)/S)\n",
    "\n",
    "    return t1 * t2\n",
    "\n",
    "\n",
    "def get_RADs(path, name, closedref=True):\n",
    "\n",
    "    # Get rank-abundance distributions, i.e, abundance vectors\n",
    "    \n",
    "    RADdict = {}\n",
    "    DATA = path + name + '-data.txt'\n",
    "\n",
    "    with open(DATA) as f:\n",
    "\n",
    "        for d in f:\n",
    "\n",
    "            if d.strip():\n",
    "                d = d.split()\n",
    "                length = len(d)\n",
    "\n",
    "                if name == 'GENTRY':\n",
    "                    site = d[0]\n",
    "                    #species = d[1] # Dataset name plus species identifier\n",
    "                    abundance = float(d[-1])\n",
    "\n",
    "                else:\n",
    "                    site = d[0]\n",
    "                    #year = d[1]\n",
    "\n",
    "                    if closedref == True:\n",
    "                        for i in d:\n",
    "                            if 'unclassified' in i:\n",
    "                                #print('unclassified')\n",
    "                                continue\n",
    "                            elif 'unidentified' in i:\n",
    "                                #print('unidentified')\n",
    "                                continue\n",
    "\n",
    "                    abundance = float(d[-1])\n",
    "\n",
    "\n",
    "                if abundance > 0:\n",
    "                    if site in RADdict:\n",
    "                        RADdict[site].append(abundance)\n",
    "                    else:\n",
    "                        RADdict[site] = [abundance]\n",
    "\n",
    "    RADs = RADdict.values()\n",
    "    filteredRADs = []\n",
    "    for rad in RADs:\n",
    "        if len(rad) >= 10:\n",
    "            filteredRADs.append(rad)\n",
    "\n",
    "    return filteredRADs\n",
    "\n",
    "\n",
    "\n",
    "def EMP_RADs(path, name):\n",
    "\n",
    "    minS = 10\n",
    "\n",
    "    IN = path + '/EMPclosed-SADs.txt'\n",
    "    rads = []\n",
    "    \n",
    "    with open(IN) as f:\n",
    "\n",
    "        for rad in f:\n",
    "            rad = eval(rad)\n",
    "            if len(rad) >= minS:\n",
    "                rads.append(rad)\n",
    "\n",
    "    return rads\n",
    "\n",
    "def Louca_RADs(path, name):\n",
    "    \n",
    "    # Get rank-abundance distributions, i.e, abundance vectors\n",
    "    \n",
    "    RADdict = {}\n",
    "    DATA = path + 'SSADdata.txt'\n",
    "\n",
    "    with open(DATA) as f:\n",
    "\n",
    "        for d in f:\n",
    "\n",
    "            if d.strip():\n",
    "                d = d.split()\n",
    "                length = len(d)\n",
    "\n",
    "                site = d[1]\n",
    "                for i in d:\n",
    "                    abundance = float(d[-1])\n",
    "\n",
    "\n",
    "                if abundance > 0:\n",
    "                    if site in RADdict:\n",
    "                        RADdict[site].append(abundance)\n",
    "                    else:\n",
    "                        RADdict[site] = [abundance]\n",
    "\n",
    "    RADs = RADdict.values()\n",
    "    filteredRADs = []\n",
    "    for rad in RADs:\n",
    "        if len(rad) >= 10:\n",
    "            filteredRADs.append(rad)\n",
    "\n",
    "    return filteredRADs\n",
    "\n",
    "\n",
    "def NSECF(p):\n",
    "    p_bins = len(p)\n",
    "    p_obs = sum(p)\n",
    "    \n",
    "    z_p = (p_bins + 1)/p_bins\n",
    "    p = [sum(p[:ii+1])**(z_p) for ii in range(len(p))]\n",
    "    return np.sum(np.array(p)/(p_obs**z_p)) - 1\n",
    "    \n",
    "                \n",
    "def getMetrics():\n",
    "    \n",
    "    name_ls = []\n",
    "    kind_ls = []\n",
    "    N_ls = []\n",
    "    S_ls = []\n",
    "    skew_ls = []\n",
    "    logskew_ls = []\n",
    "    log_mod_skew_ls = []\n",
    "    log_mod_skew_log_ls = []\n",
    "    ds_ls = []\n",
    "    nsecf_ls = []\n",
    "    \n",
    "    datasets = []\n",
    "    for name in os.listdir('data/ecological/micro'):\n",
    "            datasets.append([name, 'micro'])\n",
    "    for name in os.listdir('data/ecological/macro'):\n",
    "            datasets.append([name, 'macro'])\n",
    "\n",
    "    for dataset in datasets:\n",
    "        \n",
    "        name = dataset[0] # name of dataset\n",
    "        kind = dataset[1] # micro or macro\n",
    "\n",
    "        if name == '.DS_Store' or name == 'MGRAST': \n",
    "            continue\n",
    "        \n",
    "        OUT = open('data/ecological/'+kind+'/'+name+'/'+name+'-SADMetricData.txt','w+')\n",
    "        RADs = []\n",
    "\n",
    "        if kind == 'macro':\n",
    "            RADs = get_RADs('data/ecological/'+kind+'/'+name+'/', name)\n",
    "            print('macro', name, len(RADs))\n",
    "\n",
    "\n",
    "        if kind == 'micro':\n",
    "            if name == 'EMPclosed' or name == 'EMPopen':\n",
    "                RADs = EMP_RADs('data/ecological/'+kind+'/'+name+'/', name)\n",
    "            \n",
    "            elif name == 'Louca':\n",
    "                RADs = Louca_RADs('data/ecological/'+kind+'/'+name+'/', name)\n",
    "            \n",
    "            else:\n",
    "                RADs = get_RADs('data/ecological/'+kind+'/'+name+'/', name)\n",
    "\n",
    "            print('micro', name, len(RADs))\n",
    "\n",
    "        ct = 0\n",
    "        numRADs = len(RADs)\n",
    "        for RAD in RADs:\n",
    "\n",
    "            if kind == 'micro':\n",
    "                RAD = list([x for x in RAD if x > 0])\n",
    "\n",
    "            elif kind == 'macro':\n",
    "                RAD = list([x for x in RAD if x > 0])\n",
    "\n",
    "\n",
    "            N = sum(RAD)\n",
    "            S = len(RAD)\n",
    "\n",
    "            if S < 10: \n",
    "                continue\n",
    "            if max(RAD) == min(RAD): \n",
    "                continue\n",
    "\n",
    "            # Measures of Rarity\n",
    "            \n",
    "            # 1. skewness of abundances\n",
    "            skewness = skew(RAD)\n",
    "            \n",
    "            # 2. log-modulo transformation of skewnness\n",
    "            lms = np.log10(np.abs(float(skewness)) + 1)\n",
    "            if skewness < 0: \n",
    "                lms = lms * -1\n",
    "            log_mod_skew = float(lms)\n",
    "            \n",
    "            # 3. skewness of log-transformed abundances\n",
    "            logskew = Rlogskew(RAD)\n",
    "            \n",
    "            # 4. log-modulo transformation of skewnness of log-transformed abundances\n",
    "            lms = np.log10(np.abs(float(logskew)) + 1)\n",
    "            if skewness < 0: \n",
    "                lms = lms * -1\n",
    "            log_mod_skew_log = float(lms)\n",
    "            \n",
    "            # 5. Distributional shift (DS)\n",
    "            # Convert the abundances to logarithmic scale (base 2)\n",
    "            abundances = np.log2(RAD)\n",
    "\n",
    "            # Define the bins for the histogram\n",
    "            min_abundance = 0 #np.floor(min(abundances))\n",
    "            max_abundance = np.ceil(max(abundances))\n",
    "            bins = np.arange(min_abundance, max_abundance + 1, 1)\n",
    "\n",
    "            # Compute the histogram\n",
    "            hist, bin_edges = np.histogram(abundances, bins=bins)\n",
    "\n",
    "            # Use the right side of the bin edges as bin values\n",
    "            bin_values = bin_edges[1:]\n",
    "\n",
    "            # Convert histogram to list\n",
    "            bin_heights = hist.tolist()\n",
    "            ds = DS(bin_heights)\n",
    "            \n",
    "            # 6. Normalized sums of exponentiated cumulative frequencies\n",
    "            nsecf = NSECF(bin_heights)\n",
    "\n",
    "            ct+=1\n",
    "\n",
    "            print(name, kind, N, S, skewness, logskew, log_mod_skew, log_mod_skew_log, ds, nsecf, file=OUT)\n",
    "            \n",
    "            name_ls.append(name)\n",
    "            kind_ls.append(kind)\n",
    "            N_ls.append(N)\n",
    "            S_ls.append(S)\n",
    "            skew_ls.append(skewness)\n",
    "            logskew_ls.append(logskew)\n",
    "            log_mod_skew_ls.append(log_mod_skew)\n",
    "            log_mod_skew_log_ls.append(log_mod_skew_log)\n",
    "            ds_ls.append(ds)\n",
    "            nsecf_ls.append(nsecf)\n",
    "            \n",
    "        OUT.close()\n",
    "        \n",
    "    return name_ls, kind_ls, N_ls, S_ls, skew_ls, logskew_ls, log_mod_skew_ls, log_mod_skew_log_ls, ds_ls, nsecf_ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7874d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_ls, kind_ls, N_ls, S_ls, skew_ls, logskew_ls, log_mod_skew_ls, log_mod_skew_log_ls, ds_ls, nsecf_ls = getMetrics()\n",
    "\n",
    "main_df = pd.DataFrame({\n",
    "    'name': name_ls,\n",
    "    'kind': kind_ls,\n",
    "    'N': N_ls,\n",
    "    'S': S_ls,\n",
    "    'skew': skew_ls,\n",
    "    'logskew': logskew_ls,\n",
    "    'log_mod_skew': log_mod_skew_ls,\n",
    "    'log_mod_skew_log': log_mod_skew_log_ls,\n",
    "    'DS': ds_ls,\n",
    "    'NSECF': nsecf_ls,\n",
    "})\n",
    "\n",
    "print(main_df.shape)\n",
    "\n",
    "print(main_df['name'].unique().tolist())\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d251120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(main_df.shape)\n",
    "\n",
    "its = 20\n",
    "tdf = None\n",
    "names = ['CHU', 'CATLIN', 'BOVINE', 'LAUB', 'SED', 'BIGN', \n",
    "         'CHINA', 'TARA', 'HMP', 'FUNGI', 'Louca', 'HUMAN', \n",
    "         'HYDRO', 'EMPclosed', 'FIA', 'CBC', 'MCDB', 'BBS', \n",
    "         'GENTRY']\n",
    "\n",
    "for n in range(its):\n",
    "\n",
    "    for i, name in enumerate(names):\n",
    "                \n",
    "        tdf_nm = main_df[main_df['name'] == name]\n",
    "        kind = tdf_nm['kind'].iloc[0]\n",
    "        numlines = tdf_nm.shape[0]\n",
    "                \n",
    "        small = ['BIGN', 'BOVINE', 'CHU', 'LAUB', 'SED']\n",
    "        big = ['HUMAN', 'CHINA', 'CATLIN', 'FUNGI', 'HYDRO']\n",
    "\n",
    "        if name == 'Louca':\n",
    "            tdf_nm = tdf_nm.sample(1000, replace=True)\n",
    "\n",
    "        elif kind == 'macro':\n",
    "            tdf_nm = tdf_nm.sample(100, replace=True)\n",
    "        elif name in small:\n",
    "            tdf_nm = tdf_nm.sample(20, replace=True)\n",
    "        elif name in big:\n",
    "            tdf_nm = tdf_nm.sample(50, replace=True)\n",
    "        elif name == 'TARA':\n",
    "            tdf_nm = tdf_nm.sample(50, replace=True)\n",
    "        else:\n",
    "            tdf_nm = tdf_nm.sample(50, replace=True)\n",
    "            \n",
    "        if n == 0 and i == 0:\n",
    "            tdf = tdf_nm.copy(deep=True)\n",
    "        else:\n",
    "            tdf = pd.concat([tdf, tdf_nm])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c182bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "\n",
    "tdf.dropna(subset=['DS'], how='any', inplace=True)\n",
    "\n",
    "# Log scale the 'N' values\n",
    "log_N = np.log10(tdf['N'])\n",
    "\n",
    "# log_mod_skew values\n",
    "log_mod_skew = tdf['log_mod_skew']\n",
    "\n",
    "# DS values\n",
    "DS_vals = tdf['DS']\n",
    "\n",
    "# NSECF\n",
    "NSECF_vals = tdf['NSECF']\n",
    "\n",
    "# Reshape the data to fit the model\n",
    "X = log_N.values\n",
    "y1 = log_mod_skew.values\n",
    "y2 = DS_vals.values\n",
    "y3 = NSECF_vals.values\n",
    "\n",
    "# Fit the linear regression model for log_mod_skew\n",
    "slope1, intercept1, r_value1, p1, se1 = linregress(X, y1)\n",
    "print(\"Regression for log_mod_skew:\")\n",
    "print(\"Slope:\", slope1)\n",
    "print(\"Intercept:\", intercept1)\n",
    "print(\"r2:\", r_value1 ** 2, '\\n')\n",
    "\n",
    "# Fit the linear regression model for DS\n",
    "slope2, intercept2, r_value2, p2, se2 = linregress(X, y2)\n",
    "print(\"Regression for DS:\")\n",
    "print(\"Slope:\", slope2)\n",
    "print(\"Intercept:\", intercept2)\n",
    "print(\"r2:\", r_value2 ** 2, '\\n')\n",
    "\n",
    "# Fit the linear regression model for NSECF\n",
    "slope3, intercept3, r_value3, p3, se3 = linregress(X, y3)\n",
    "print(\"Regression for NSECF:\")\n",
    "print(\"Slope:\", slope3)\n",
    "print(\"Intercept:\", intercept3)\n",
    "print(\"r2:\", r_value3 ** 2)\n",
    "\n",
    "# Plotting\n",
    "fig = plt.figure(figsize=(10, 2.5))\n",
    "x_lab = 'log(N)'\n",
    "\n",
    "# Scatter plot and regression line for log_mod_skew\n",
    "plot_color_by_pt_dens(X, y1, radius=0.05, loglog=0, plot_obj=plt.subplot(1, 3, 1), point_size=10)\n",
    "slope, intercept, r_val, p_val, std_err = linregress(X, y1)\n",
    "fitted_vals = slope * np.array(X) + intercept\n",
    "s = r'$r_{2}$' + ' = ' + str(round(r_val**2, 2))\n",
    "plt.plot(X, fitted_vals, color='k', linewidth=2, label=s)\n",
    "plt.xlabel(x_lab, fontweight='bold')\n",
    "plt.ylabel('log-modulo skewness', fontweight='bold')\n",
    "plt.title(r'Rarity = '+str(round(10**intercept,2))+'*'+r'$N$'+'$^{'+str(round(slope,2))+'}$')\n",
    "plt.legend(frameon=False)\n",
    "\n",
    "# Scatter plot and regression line for DS (non-standardized)\n",
    "plot_color_by_pt_dens(X, y3, radius=0.05, loglog=0, plot_obj=plt.subplot(1, 3, 2), point_size=10)\n",
    "slope, intercept, r_val, p_val, std_err = linregress(X, y3)\n",
    "fitted_vals = slope * np.array(X) + intercept\n",
    "s = r'$r_{2}$' + ' = ' + str(round(r_val**2, 2))\n",
    "plt.plot(X, fitted_vals, color='k', linewidth=2, label=s)\n",
    "plt.xlabel(x_lab, fontweight='bold')\n",
    "plt.ylabel('DS, non-standardized', fontweight='bold')\n",
    "plt.title(r'Rarity = '+str(round(slope,2))+'*'+r'$N$'+ ' - ' + str(abs(round(intercept,2))))\n",
    "plt.legend(frameon=False)\n",
    "\n",
    "\n",
    "# Scatter plot and regression line for DS\n",
    "plot_color_by_pt_dens(X, y2, radius=0.05, loglog=0, plot_obj=plt.subplot(1, 3, 3), point_size=10)\n",
    "slope, intercept, r_val, p_val, std_err = linregress(X, y2)\n",
    "fitted_vals = slope * np.array(X) + intercept\n",
    "s = r'$r_{2}$' + ' = ' + str(round(r_val**2, 2))\n",
    "plt.plot(X, fitted_vals, color='k', linewidth=2, label=s)\n",
    "plt.xlabel(x_lab, fontweight='bold')\n",
    "plt.ylabel('DS, standardized', fontweight='bold')\n",
    "plt.legend(frameon=False)\n",
    "\n",
    "\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.subplots_adjust(wspace=0.45, hspace=0.35)\n",
    "plt.savefig('Fig8.jpg', bbox_inches='tight', format='jpg', dpi=600)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422d41f5",
   "metadata": {},
   "source": [
    "#   \n",
    "\n",
    "# ECONOMIC ANALYSIS\n",
    "\n",
    "## FAMILY INCOME AND POVERTY BY RACE/ETHNICITY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb28a2a",
   "metadata": {},
   "source": [
    "## Family income data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240d308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for name in os.listdir('data/economic/USCB/FAMILY_INCOME_IN_THE_PAST_12_MONTHS/data/'):\n",
    "    datasets.append(name)\n",
    "\n",
    "main_df = pd.DataFrame(columns=['Year'])\n",
    "\n",
    "yrs = ['5Y2010', '5Y2011', '5Y2012', '5Y2013', '5Y2014', \n",
    "       '5Y2015', '5Y2016', '5Y2017', '5Y2018', '5Y2019', '5Y2020', \n",
    "       '5Y2021', '5Y2022']\n",
    "\n",
    "# Iterate over each year to concatenate files and add a 'year' column\n",
    "\n",
    "for yr in yrs:\n",
    "    tdf = pd.DataFrame(columns=['Year'])\n",
    "\n",
    "    files = []\n",
    "    for d in datasets:\n",
    "        if yr in d:\n",
    "            files.append(d)\n",
    "\n",
    "    for file in files:\n",
    "        # Read CSV and specify the header row\n",
    "        df = pd.read_csv('data/economic/USCB/FAMILY_INCOME_IN_THE_PAST_12_MONTHS/data/'+file, header=0)\n",
    "        \n",
    "        # Reset the index of the DataFrame\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        lab = file\n",
    "        lab = lab.replace('-Data.csv', '')\n",
    "        lab = lab + '-Table-Notes.txt'\n",
    "\n",
    "        path = 'data/economic/USCB/FAMILY_INCOME_IN_THE_PAST_12_MONTHS/notes/'+lab\n",
    "        # Open the file in read mode\n",
    "        with open(path, \"r\") as File:\n",
    "            # Read the first six lines\n",
    "            first_six_lines = [File.readline() for _ in range(6)]\n",
    "\n",
    "        # Print the first six lines\n",
    "        lab1 = first_six_lines[-1]\n",
    "        lab1 = lab1.upper()\n",
    "\n",
    "        R_E = ['TWO OR MORE RACES HOUSEHOLDER',\n",
    "            'BLACK OR AFRICAN AMERICAN ALONE HOUSEHOLDER',\n",
    "            'WHITE ALONE HOUSEHOLDER',\n",
    "            'ASIAN ALONE HOUSEHOLDER',\n",
    "            'HISPANIC OR LATINO HOUSEHOLDER',\n",
    "            'WHITE ALONE, NOT HISPANIC OR LATINO HOUSEHOLDER',\n",
    "            'NATIVE HAWAIIAN AND OTHER PACIFIC ISLANDER ALONE HOUSEHOLDER',\n",
    "            'AMERICAN INDIAN AND ALASKA NATIVE ALONE HOUSEHOLDER',\n",
    "           ]\n",
    "\n",
    "        lab2 = str(lab1)\n",
    "        for r_e in R_E:\n",
    "            if r_e in lab1:\n",
    "                lab2 = r_e\n",
    "\n",
    "        if lab2 == lab1:\n",
    "            lab2 = 'ALL'\n",
    "\n",
    "        # Set new headers, drop redundant rows, and reset index\n",
    "        new_headers = df.iloc[0]\n",
    "        df.columns = new_headers\n",
    "        df = df.drop(0)\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        df = df.drop(columns=['Geography', 'Geographic Area Name'])\n",
    "        df = df.dropna(axis=1, how='all')\n",
    "\n",
    "        df.dropna(how='all', axis=1, inplace=True)\n",
    "        cols_to_drop = df.filter(regex='Margin of Error').columns\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        cols_to_drop = df.filter(regex='Median').columns\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        cols_to_drop = df.filter(regex='MEDIAN').columns\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        cols_to_drop = df.filter(regex='Mean').columns\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        cols_to_drop = df.filter(regex='MEAN').columns\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        cols_to_drop = df.filter(regex='IMPUTED').columns\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        \n",
    "        df.columns = df.columns.str.replace('Total:', 'Total')\n",
    "        df.columns = df.columns.str.replace(\"Estimate!!\", \"\")\n",
    "        df.columns = df.columns.str.replace(\"Total!!\", \"\")\n",
    "        \n",
    "        df.columns = [col + ' - ' + lab2 for col in df.columns]\n",
    "        df['Year'] = yr[2:]\n",
    "        \n",
    "        df.columns = df.columns.str.replace(\" HOUSEHOLDER\", \"\")\n",
    "        df.columns = df.columns.str.replace(\" ALONE\", \"\")\n",
    "        \n",
    "        # Merge files for same year\n",
    "        tdf = tdf.merge(df, how='outer', on='Year')\n",
    "        tdf.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    # concat different years\n",
    "    try:\n",
    "        main_df = pd.concat([main_df, tdf])\n",
    "        #main_df = main_df.sort_index(axis=1)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "print(main_df.shape)\n",
    "main_df.head(main_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676524d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "suf_ls = ['WHITE',\n",
    "          'HISPANIC OR LATINO',\n",
    "          'WHITE, NOT HISPANIC OR LATINO',\n",
    "          'BLACK OR AFRICAN AMERICAN',\n",
    "          'ASIAN',\n",
    "          'ALL',\n",
    "          'NATIVE HAWAIIAN AND OTHER PACIFIC ISLANDER',\n",
    "          'AMERICAN INDIAN AND ALASKA NATIVE',\n",
    "         ]\n",
    "\n",
    "for suf in suf_ls:\n",
    "    # Define columns to convert to float\n",
    "\n",
    "    print(suf)\n",
    "    cols = ['Less than $10,000 - '+suf, \n",
    "        '$10,000 to $14,999 - '+suf, \n",
    "        '$15,000 to $19,999 - '+suf, \n",
    "        '$20,000 to $24,999 - '+suf, \n",
    "        '$25,000 to $29,999 - '+suf, \n",
    "        '$30,000 to $34,999 - '+suf, \n",
    "        '$35,000 to $39,999 - '+suf, \n",
    "        '$40,000 to $44,999 - '+suf, \n",
    "        '$45,000 to $49,999 - '+suf, \n",
    "        '$50,000 to $59,999 - '+suf, \n",
    "        '$60,000 to $74,999 - '+suf, \n",
    "        '$75,000 to $99,999 - '+suf, \n",
    "        '$100,000 to $124,999 - '+suf, \n",
    "        '$125,000 to $149,999 - '+suf, \n",
    "        '$150,000 to $199,999 - '+suf, \n",
    "        '$200,000 or more - '+suf,\n",
    "       ]\n",
    "\n",
    "    main_df[cols] = main_df[cols].astype(float)\n",
    "    main_df['DOW'] = main_df.apply(lambda row: row[cols].tolist(), axis=1)\n",
    "\n",
    "    ds_ls = []\n",
    "    for dow in main_df['DOW'].tolist():\n",
    "        ds = DS(dow)\n",
    "        ds_ls.append(ds)\n",
    "\n",
    "    main_df[suf+'_DS'] = ds_ls\n",
    "    main_df.drop(labels=['DOW'], axis=1, inplace=True)\n",
    "    \n",
    "    main_df[suf + ' < $25K'] = 100 * (main_df[cols[0]] + main_df[cols[1]] + main_df[cols[2]] + main_df[cols[3]]) / main_df['Total - ' + suf].astype(float)\n",
    "    main_df[suf + ' ≥ $200K'] = 100 * (main_df[cols[-1]]) / main_df['Total - ' + suf].astype(float)\n",
    "\n",
    "    ##\n",
    "    ## Get DS for lower\n",
    "    ##\n",
    "    \n",
    "    cols = ['Less than $10,000 - '+suf, \n",
    "        '$10,000 to $14,999 - '+suf, \n",
    "        '$15,000 to $19,999 - '+suf, \n",
    "        '$20,000 to $24,999 - '+suf, \n",
    "        ]\n",
    "\n",
    "    main_df['DOW'] = main_df.apply(lambda row: row[cols].tolist(), axis=1)\n",
    "\n",
    "    ds_ls = []\n",
    "    for dow in main_df['DOW'].tolist():\n",
    "        ds = DS(dow)\n",
    "        ds_ls.append(ds)\n",
    "\n",
    "    main_df[suf+'_DS_lower'] = ds_ls\n",
    "    main_df.drop(labels=['DOW'], axis=1, inplace=True)\n",
    "    \n",
    "    ##\n",
    "    ## Get DS for middle\n",
    "    ##\n",
    "    \n",
    "    cols = ['$25,000 to $29,999 - '+suf, \n",
    "        '$30,000 to $34,999 - '+suf, \n",
    "        '$35,000 to $39,999 - '+suf, \n",
    "        '$40,000 to $44,999 - '+suf,\n",
    "        '$45,000 to $49,999 - '+suf, \n",
    "        '$50,000 to $59,999 - '+suf, \n",
    "        '$60,000 to $74,999 - '+suf, \n",
    "        '$75,000 to $99,999 - '+suf, \n",
    "       ]\n",
    "\n",
    "    main_df['DOW'] = main_df.apply(lambda row: row[cols].tolist(), axis=1)\n",
    "\n",
    "    ds_ls = []\n",
    "    for dow in main_df['DOW'].tolist():\n",
    "        ds = DS(dow)\n",
    "        ds_ls.append(ds)\n",
    "\n",
    "    main_df[suf+'_DS_middle'] = ds_ls\n",
    "    main_df.drop(labels=['DOW'], axis=1, inplace=True)\n",
    "    \n",
    "    ##\n",
    "    ## Get DS for upper\n",
    "    ##\n",
    "    \n",
    "    cols = [\n",
    "        '$100,000 to $124,999 - '+suf, \n",
    "        '$125,000 to $149,999 - '+suf, \n",
    "        '$150,000 to $199,999 - '+suf, \n",
    "        '$200,000 or more - '+suf,\n",
    "       ]\n",
    "    \n",
    "    main_df['DOW'] = main_df.apply(lambda row: row[cols].tolist(), axis=1)\n",
    "\n",
    "    ds_ls = []\n",
    "    for dow in main_df['DOW'].tolist():\n",
    "        ds = DS(dow)\n",
    "        ds_ls.append(ds)\n",
    "\n",
    "    main_df[suf+'_DS_upper'] = ds_ls\n",
    "    main_df.drop(labels=['DOW'], axis=1, inplace=True)\n",
    "\n",
    "main_df.head(main_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6800d6cf",
   "metadata": {},
   "source": [
    "## Poverty data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206e3910",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for name in os.listdir('data/economic/USCB/POVERTY/data/'):\n",
    "    datasets.append(name)\n",
    "\n",
    "df2 = pd.DataFrame(columns=['Year'])\n",
    "\n",
    "yrs = ['1Y2010', '1Y2011', '1Y2012', '1Y2013', \n",
    "       '1Y2014', '1Y2015', '1Y2016', \n",
    "       '5Y2017', '5Y2018', '5Y2019', '5Y2020', \n",
    "       '5Y2021', '5Y2022',\n",
    "      ]\n",
    "\n",
    "# Iterate over each year to concatenate files and add a 'year' column\n",
    "\n",
    "for yr in yrs:\n",
    "    tdf = pd.DataFrame(columns=['Year'])\n",
    "\n",
    "    files = []\n",
    "    for d in datasets:\n",
    "        if yr in d:\n",
    "            files.append(d)\n",
    "\n",
    "    for file in files:\n",
    "        # Read CSV file and specify header row\n",
    "        df = pd.read_csv('data/economic/USCB/POVERTY/data/'+file, header=0)\n",
    "        \n",
    "        # Reset index of DataFrame\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        # Set new headers, drop redundant rows, and reset index\n",
    "        new_headers = df.iloc[0]\n",
    "        df.columns = new_headers\n",
    "        df = df.drop(0)\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "        df.columns = df.columns.str.replace('One race!!White', 'White')\n",
    "        df.columns = df.columns.str.replace('One race!!Black or African American', 'Black or African American')\n",
    "        df.columns = df.columns.str.replace('One race!!American Indian and Alaska Native', 'American Indian and Alaska Native')\n",
    "        df.columns = df.columns.str.replace('One race!!Asian', 'Asian')\n",
    "        df.columns = df.columns.str.replace('One race!!Native Hawaiian and Other Pacific Islander', 'Native Hawaiian and Other Pacific Islander')\n",
    "        \n",
    "        df.columns = df.columns.str.replace('White alone', 'White')\n",
    "        df.columns = df.columns.str.replace('Black or African American alone', 'Black or African American')\n",
    "        df.columns = df.columns.str.replace('American Indian and Alaska Native alone', 'American Indian and Alaska Native')\n",
    "        df.columns = df.columns.str.replace('Asian alone', 'Asian')\n",
    "        df.columns = df.columns.str.replace('Native Hawaiian and Other Pacific Islander alone', 'Native Hawaiian and Other Pacific Islander')\n",
    "        \n",
    "        df.columns = df.columns.str.replace(\"\\(of any race\\)\", \"\")\n",
    "        \n",
    "        df.columns = df.columns.str.replace(\"Estimate!!Total!!\", \"Total!!Estimate!!\")\n",
    "        df.columns = df.columns.str.replace(\"Estimate!!Below poverty level!!\", \"Below poverty level!!Estimate!!\")\n",
    "        \n",
    "        df.columns = df.columns.str.replace(\"Population for whom poverty status is determined!!\", \"\")\n",
    "        \n",
    "        df.columns = df.columns.str.replace(\"RACE AND HISPANIC OR LATINO ORIGIN!!\", \"\")\n",
    "        \n",
    "        \n",
    "        df = df.drop(columns=['Geography', 'Geographic Area Name'])\n",
    "        cols_to_drop = df.filter(regex='Margin of Error').columns\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        cols_to_drop = df.filter(regex='Median').columns\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        cols_to_drop = df.filter(regex='MEDIAN').columns\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        cols_to_drop = df.filter(regex='Mean').columns\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        cols_to_drop = df.filter(regex='MEAN').columns\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        cols_to_drop = df.filter(regex='IMPUTED').columns\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        cols_to_drop = df.filter(regex='years').columns\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        cols_to_drop = df.filter(regex='Female').columns\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        cols_to_drop = df.filter(regex='Male').columns\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        cols_to_drop = df.filter(regex='housing').columns\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        cols_to_drop = df.filter(regex='ork').columns\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        cols_to_drop = df.filter(regex='raduate').columns\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        cols_to_drop = df.filter(regex='egree').columns\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        cols_to_drop = df.filter(regex='ndividuals').columns\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        cols_to_drop = df.filter(regex='NDIVIDUALS').columns\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        cols_to_drop = df.filter(regex='mploy').columns\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        cols_to_drop = df.filter(regex='other').columns\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        cols_to_drop = df.filter(regex='or more race').columns\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        cols_to_drop = df.filter(regex='One race').columns\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        cols_to_drop = df.filter(regex='Percent').columns\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        \n",
    "        df.dropna(how='all', axis=1, inplace=True)\n",
    "        #df.columns = df.columns.str.replace('Total:', 'Total')\n",
    "\n",
    "        df['Year'] = yr[2:]\n",
    "\n",
    "        # Merge files for same year\n",
    "        tdf = tdf.merge(df, how='outer', on='Year')\n",
    "        tdf.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    # Now concat different years\n",
    "    try:\n",
    "        df2 = pd.concat([df2, tdf])\n",
    "        #main_df = main_df.sort_index(axis=1)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(df2.shape)\n",
    "df2.head(df2.shape[0])\n",
    "\n",
    "\n",
    "suf1_ls = ['White',\n",
    "           'Hispanic or Latino origin ',\n",
    "           'White, not Hispanic or Latino',\n",
    "           'Black or African American',\n",
    "           'Asian',\n",
    "           'Population for whom poverty status is determined',\n",
    "           'Native Hawaiian and Other Pacific Islander',\n",
    "           'American Indian and Alaska Native',\n",
    "          ]\n",
    "\n",
    "suf2_ls = ['WHITE',\n",
    "          'HISPANIC OR LATINO',\n",
    "          'WHITE, NOT HISPANIC OR LATINO',\n",
    "          'BLACK OR AFRICAN AMERICAN',\n",
    "          'ASIAN',\n",
    "          'ALL',\n",
    "          'NATIVE HAWAIIAN AND OTHER PACIFIC ISLANDER',\n",
    "          'AMERICAN INDIAN AND ALASKA NATIVE',\n",
    "         ]\n",
    "\n",
    "        \n",
    "for i, suf in enumerate(suf1_ls):\n",
    "    \n",
    "    pvr = 100 * df2['Below poverty level!!Estimate!!' + suf].astype(float) / df2['Total!!Estimate!!' + suf].astype(float)\n",
    "    \n",
    "    main_df[suf2_ls[i] + ' - % Poverty'] = pvr\n",
    "    \n",
    "main_df.head(main_df.shape[0])     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d912e39",
   "metadata": {},
   "source": [
    "### FIGURE 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e89c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_colors = {\n",
    "    'BLACK OR AFRICAN AMERICAN': '0.8',\n",
    "    'AMERICAN INDIAN AND ALASKA NATIVE': '0.8',\n",
    "    'HISPANIC OR LATINO': '0.8',\n",
    "    'ALL': 'k',\n",
    "    'WHITE, NOT HISPANIC OR LATINO': '0.1',\n",
    "    'ASIAN': '0.1',\n",
    "    #'NATIVE HAWAIIAN AND OTHER PACIFIC ISLANDER': '0.5',\n",
    "    \n",
    "}\n",
    "\n",
    "race_styles = {\n",
    "    'BLACK OR AFRICAN AMERICAN': 'solid',\n",
    "    'AMERICAN INDIAN AND ALASKA NATIVE': 'dashed',\n",
    "    'HISPANIC OR LATINO': 'dotted',\n",
    "    'ALL': 'dotted',\n",
    "    'WHITE, NOT HISPANIC OR LATINO': 'solid',\n",
    "    'ASIAN': 'dashed',\n",
    "    #'NATIVE HAWAIIAN AND OTHER PACIFIC ISLANDER': '0.5',\n",
    "}\n",
    "\n",
    "race_labels = {\n",
    "    'HISPANIC OR LATINO': 'Hispanic or Latinx',\n",
    "    'WHITE, NOT HISPANIC OR LATINO': 'White, non-hispanic/latinx',\n",
    "    'BLACK OR AFRICAN AMERICAN': 'Black or African American',\n",
    "    'ASIAN': 'Asian',\n",
    "    'ALL': 'All',\n",
    "    'NATIVE HAWAIIAN AND OTHER PACIFIC ISLANDER': 'Native Hawaiian or Pacific Islander',\n",
    "    'AMERICAN INDIAN AND ALASKA NATIVE': 'Native American or Alaska Native',\n",
    "}\n",
    "\n",
    "# Use the race/ethnicity labels\n",
    "race_eth = ['AMERICAN INDIAN AND ALASKA NATIVE',\n",
    "            'BLACK OR AFRICAN AMERICAN',\n",
    "            'HISPANIC OR LATINO',\n",
    "            #'ALL',\n",
    "            'WHITE, NOT HISPANIC OR LATINO',\n",
    "            'ASIAN',\n",
    "            #'NATIVE HAWAIIAN AND OTHER PACIFIC ISLANDER',\n",
    "           ]\n",
    "  \n",
    "# Use these years\n",
    "years = ['2010', '', '2012', '', '2014', \n",
    "         '', '2016', '', '2018', '', '2020',\n",
    "         '', '2022',\n",
    "        ]\n",
    "\n",
    "\n",
    "# Create figure and axis objects\n",
    "fig, axs = plt.subplots(1, 3, figsize=(13, 4))\n",
    "\n",
    "# Plot poverty vs year\n",
    "c = 0\n",
    "for race in race_eth:\n",
    "    axs[c].plot(main_df['Year'], main_df[f'{race} - % Poverty'], \n",
    "                label=race_labels[race], \n",
    "                color=race_colors[race], \n",
    "                linewidth=3, \n",
    "                linestyle=race_styles[race],\n",
    "               )\n",
    "    \n",
    "axs[c].set_xlabel('Year', fontsize=14, fontweight='bold')\n",
    "axs[c].set_ylabel('US Poverty Rate (%)', fontsize=14, fontweight='bold')\n",
    "axs[c].tick_params(axis='x', rotation=45) \n",
    "axs[c].set_xticklabels(years) \n",
    "\n",
    "# Plot DS vs year\n",
    "c = 1\n",
    "for race in race_eth:\n",
    "    axs[c].plot(main_df['Year'], main_df[f'{race}_DS'], \n",
    "                color=race_colors[race], \n",
    "                linewidth=3, \n",
    "                linestyle=race_styles[race],\n",
    "               )\n",
    "    \n",
    "axs[c].set_xlabel('Year', fontsize=14, fontweight='bold')\n",
    "axs[c].set_ylabel('US Poverty (DS)', fontsize=14, fontweight='bold')\n",
    "axs[c].tick_params(axis='x', rotation=45)\n",
    "axs[c].set_xticklabels(years) \n",
    "\n",
    "\n",
    "# Plot poverty vs year\n",
    "c = 2\n",
    "for race in race_eth:\n",
    "    axs[c].plot(main_df['Year'], main_df[f'{race}_DS_lower'], \n",
    "                color=race_colors[race], \n",
    "                linewidth=3,\n",
    "                linestyle=race_styles[race],\n",
    "               )\n",
    "axs[c].set_xlabel('Year', fontsize=14, fontweight='bold')\n",
    "axs[c].set_ylabel('Poverty (DS) of families\\nmaking <$25K/year', fontsize=14, fontweight='bold')\n",
    "axs[c].tick_params(axis='x', rotation=45) \n",
    "#axs[c].set_title('Families making <$25K per year', fontsize=14, fontweight='bold')\n",
    "axs[c].set_xticklabels(years)\n",
    "\n",
    "\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.tight_layout()\n",
    "fig.legend(bbox_to_anchor=(0.055, 1.05, 0.937, .1), \n",
    "           loc=10, \n",
    "           ncol=3, \n",
    "           mode=\"expand\",\n",
    "           prop={'size':14},\n",
    "          )\n",
    "\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "plt.savefig('Fig3.jpg', bbox_inches='tight', format='jpg', dpi=600)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16abe5c6",
   "metadata": {},
   "source": [
    "### FIGURE 10\n",
    "\n",
    "### DS for lower, middle, upper classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7660a94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the figure and axis objects\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 4))\n",
    "\n",
    "# Plot poverty vs year\n",
    "c = 0\n",
    "for race in race_eth:\n",
    "    axs[c].plot(main_df['Year'], main_df[f'{race}_DS_lower'], \n",
    "                label=race_labels[race], \n",
    "                color=race_colors[race], \n",
    "                linewidth=3,\n",
    "                linestyle=race_styles[race],\n",
    "               )\n",
    "axs[c].set_xlabel('Year', fontsize=14, fontweight='bold')\n",
    "axs[c].set_ylabel('Poverty, DS', fontsize=14, fontweight='bold')\n",
    "axs[c].tick_params(axis='x', rotation=45) \n",
    "axs[c].set_title('<$25K', fontsize=14, fontweight='bold')\n",
    "axs[c].set_xticklabels(years) \n",
    "\n",
    "# Plot poverty vs year\n",
    "c = 1\n",
    "for race in race_eth:\n",
    "    axs[c].plot(main_df['Year'], main_df[f'{race}_DS_middle'],\n",
    "                color=race_colors[race], \n",
    "                linewidth=3,\n",
    "                linestyle=race_styles[race],\n",
    "               )\n",
    "axs[c].set_xlabel('Year', fontsize=14, fontweight='bold')\n",
    "axs[c].set_ylabel('Poverty, DS', fontsize=14, fontweight='bold')\n",
    "axs[c].tick_params(axis='x', rotation=45) \n",
    "axs[c].set_title('\\$25K to $100K', fontsize=14, fontweight='bold')\n",
    "axs[c].set_xticklabels(years) \n",
    "\n",
    "# Plot DS vs year\n",
    "c = 2\n",
    "for race in race_eth:\n",
    "    axs[c].plot(main_df['Year'], 1-main_df[f'{race}_DS_upper'],\n",
    "                color=race_colors[race], \n",
    "                linewidth=3,\n",
    "                linestyle=race_styles[race],\n",
    "               )\n",
    "axs[c].set_xlabel('Year', fontsize=14, fontweight='bold')\n",
    "axs[c].set_ylabel('Affluency, 1 - DS', fontsize=14, fontweight='bold')\n",
    "axs[c].tick_params(axis='x', rotation=45) \n",
    "axs[c].set_title('>$100K', fontsize=14, fontweight='bold')\n",
    "axs[c].set_xticklabels(years) \n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.legend(bbox_to_anchor=(0.0725, 1.05, 0.915, .1), \n",
    "           loc=10, \n",
    "           ncol=3, \n",
    "           mode=\"expand\",\n",
    "           prop={'size':12},\n",
    "          )\n",
    "\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "plt.savefig('Fig10.jpg', bbox_inches='tight', format='jpg', dpi=600)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaf2d94",
   "metadata": {},
   "source": [
    "##  GLOBAL FOOD COMMODITIES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0877c3db",
   "metadata": {},
   "source": [
    "## United Nations FAO data: Production of 'Crops and livestock products' in 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb92890",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/economic/FAO/FAOSTAT_data_en_2-23-2024.csv')\n",
    "\n",
    "df.drop(labels=['Domain', 'Element', 'Year'], axis=1, inplace=True)\n",
    "df = df[df['Unit'] == 't']\n",
    "\n",
    "print(len(df['Item'].unique().tolist()))\n",
    "print(len(df['Area'].unique().tolist()))\n",
    "\n",
    "#for d in sorted(df['Item'].unique().tolist()):\n",
    "#    print(d)\n",
    "    \n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7bc4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_coefficient(x):\n",
    "    \"\"\"\n",
    "    Compute Gini coefficient of array of values\n",
    "    From: https://stackoverflow.com/questions/39512260/calculating-gini-coefficient-in-python-numpy\n",
    "    \"\"\"\n",
    "    diffsum = 0\n",
    "    for i, xi in enumerate(x[:-1], 1):\n",
    "        diffsum += np.sum(np.abs(xi - x[i:]))\n",
    "    return diffsum / (len(x)**2 * np.mean(x))\n",
    "\n",
    "\n",
    "gini_ls = []\n",
    "ds_ls1 = []\n",
    "ds_ls2 = []\n",
    "food_ls = []\n",
    "\n",
    "foods = df['Item'].unique().tolist()\n",
    "\n",
    "for f in foods:\n",
    "    food_ls.append(f)\n",
    "    \n",
    "    tdf = df[df['Item'] == f]\n",
    "    tdf = tdf[tdf['Value'] > 0]\n",
    "    \n",
    "    if tdf.shape[0] == 0 or np.max(tdf['Value']) == 0:\n",
    "        continue\n",
    "    \n",
    "    vals = tdf['Value'].astype('float')\n",
    "    vals = np.array(sorted(list(vals), reverse=True))\n",
    "    \n",
    "    # Gini coefficient\n",
    "    gini_ls.append(gini_coefficient(np.sqrt(vals)))\n",
    "    #gini_ls.append(stats.entropy(vals))\n",
    "    \n",
    "    # Distributional shift (DS)\n",
    "    # 1. Convert the abundances to logarithmic scale (base 2)\n",
    "    abundances = np.log2(vals).tolist()\n",
    "\n",
    "    # 2. Define the bins for the histogram\n",
    "    min_abundance = 0\n",
    "    max_abundance = np.ceil(max(abundances))\n",
    "    bins = np.arange(min_abundance, max_abundance + 1, 1)\n",
    "\n",
    "    # 3. Compute the histogram\n",
    "    hist, bin_edges = np.histogram(abundances, bins=bins)\n",
    "\n",
    "    # 4. Use the right side of the bin edges as bin values\n",
    "    bin_values = bin_edges[1:]\n",
    "\n",
    "    # 5. Convert histogram to list\n",
    "    bin_heights = hist.tolist()\n",
    "    \n",
    "    # Calculate DS\n",
    "    ds = DS(bin_heights)\n",
    "    ds_ls1.append(ds)\n",
    "    \n",
    "    # 6. Normalized sums of exponentiated cumulative frequencies\n",
    "    nsecf = NSECF(bin_heights)\n",
    "    ds_ls2.append(nsecf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32390c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def obs_pred_rsquare(obs, pred):\n",
    "    '''\n",
    "    Determines the proportion of variability in a data set accounted for by a model\n",
    "    In other words, this determines the proportion of variation explained by the 1:1 line\n",
    "    in an observed-predicted plot.\n",
    "    \n",
    "    Used in various peer-reviewed publications:\n",
    "        1. Locey, K.J. and White, E.P., 2013. How species richness and total abundance \n",
    "        constrain the distribution of abundance. Ecology letters, 16(9), pp.1177-1185.\n",
    "        2. Xiao, X., McGlinn, D.J. and White, E.P., 2015. A strong test of the maximum \n",
    "        entropy theory of ecology. The American Naturalist, 185(3), pp.E70-E80.\n",
    "        3. Baldridge, E., Harris, D.J., Xiao, X. and White, E.P., 2016. An extensive \n",
    "        comparison of species-abundance distribution models. PeerJ, 4, p.e2823.\n",
    "    '''\n",
    "    r2 = 1 - sum((obs - pred) ** 2) / sum((obs - np.mean(obs)) ** 2)\n",
    "    return r2\n",
    "\n",
    "y_o = np.array(ds_ls1)\n",
    "x_o = np.array(gini_ls)\n",
    "\n",
    "x_o, y_o = zip(*sorted(zip(x_o, y_o)))\n",
    "                \n",
    "x_o = np.array(x_o)\n",
    "y_o = np.array(y_o)\n",
    "            \n",
    "#Create single dimension\n",
    "x = x_o[:, np.newaxis]\n",
    "y = y_o[:, np.newaxis]\n",
    "\n",
    "# Sort x values and get index\n",
    "inds = x.ravel().argsort()  \n",
    "x = x.ravel()[inds].reshape(-1, 1)\n",
    "#Sort y according to x sorted index\n",
    "y = y[inds]\n",
    "\n",
    "exp = 1\n",
    "polynomial_features = PolynomialFeatures(degree = exp)\n",
    "xp = polynomial_features.fit_transform(x)\n",
    "                    \n",
    "model = sm.OLS(y, xp).fit()\n",
    "ypred = model.predict(xp)\n",
    "ypred = ypred.tolist()\n",
    "\n",
    "\n",
    "poly_coefs = model.params[1:].tolist()\n",
    "poly_coefs.reverse()\n",
    "        \n",
    "poly_exponents = list(range(1, len(poly_coefs)+1))\n",
    "poly_exponents.reverse()\n",
    "\n",
    "eqn = 'y = '\n",
    "for i, p in enumerate(poly_coefs):\n",
    "    exp = poly_exponents[i]\n",
    "                \n",
    "    if exp == 1:\n",
    "        exp = 'x'\n",
    "    elif exp == 2:\n",
    "        exp = 'x²'\n",
    "    elif exp == 3:\n",
    "        exp = 'x³'\n",
    "            \n",
    "    if i == 0:\n",
    "        p = round(p, 4)\n",
    "        eqn = eqn + str(p) + exp\n",
    "                \n",
    "    else:\n",
    "        if p >= 0:\n",
    "            p = round(p, 4)\n",
    "            eqn = eqn + ' + ' + str(p) + exp\n",
    "        else:\n",
    "            p = round(p, 4)\n",
    "            eqn = eqn + ' - ' + str(np.abs(p)) + exp\n",
    "\n",
    "            \n",
    "            \n",
    "b = model.params[0]\n",
    "if b >= 0:\n",
    "    b = round(b, 4)\n",
    "    eqn = eqn + ' + ' + str(b)\n",
    "else:\n",
    "    b = round(b, 4)\n",
    "    eqn = eqn + ' - ' + str(np.abs(b))\n",
    "    \n",
    "print(eqn)\n",
    "\n",
    "\n",
    "try:\n",
    "    y = y.flatten().tolist()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "op_r2 = obs_pred_rsquare(np.array(y), np.array(ypred))\n",
    "\n",
    "try:\n",
    "    op_r2 = round(op_r2, 4)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if op_r2 < 0:\n",
    "    op_r2 = 0\n",
    "    \n",
    "r2 = round(model.rsquared, 4)\n",
    "r2_adj = round(model.rsquared_adj, 4)\n",
    "print(r2, r2_adj, op_r2)\n",
    "\n",
    "st, data, ss2 = summary_table(model, alpha=0.05)\n",
    "predict_mean_ci_low, predict_mean_ci_upp = data[:, 4:6].T # confidence interval\n",
    "predict_ci_low, predict_ci_upp = data[:, 6:8].T # prediction interval\n",
    "\n",
    "outlier_y = []\n",
    "outlier_x = []\n",
    "nonoutlier_y = []\n",
    "nonoutlier_x = []\n",
    "\n",
    "for i, yi in enumerate(y_o):\n",
    "    if yi > predict_ci_upp[i] or yi < predict_ci_low[i]:\n",
    "        outlier_y.append(yi)\n",
    "        outlier_x.append(x_o[i])\n",
    "    else:\n",
    "        nonoutlier_y.append(yi)\n",
    "        nonoutlier_x.append(x_o[i])\n",
    "                \n",
    "obs_pred_r2 = obs_pred_rsquare(y_o, ypred)\n",
    "obs_pred_r2 = str(np.round(obs_pred_r2, 3))\n",
    "\n",
    "print(obs_pred_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684a05e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the figure and axis objects\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "plt.plot(x_o, ypred, c='0.5', label=r'$r^{2}$' + ' = ' + obs_pred_r2)\n",
    "\n",
    "plt.fill_between(x_o, predict_ci_upp, predict_ci_low, color='k', alpha=0.1, linewidths=0)\n",
    "plt.fill_between(x_o, predict_mean_ci_upp, predict_mean_ci_low, color='k', alpha=0.2, linewidths=0)\n",
    "\n",
    "plt.scatter(nonoutlier_x, nonoutlier_y, s=5, c='k')\n",
    "plt.scatter(outlier_x, outlier_y, s=5, c='k')\n",
    "\n",
    "plt.xlabel('Inequality, (Gini Index)', fontsize= 14)\n",
    "plt.ylabel('Scarcity, (DS)', fontsize= 14)\n",
    "#plt.text(1.01, 3.8, s, fontsize=12)\n",
    "#plt.tick_params(axis='both', labelsize=10)\n",
    "#plot_num += 1\n",
    "\n",
    "plt.ylim(-0.005, .6)\n",
    "#plt.xlim(-0.005, 1.)\n",
    "plt.legend()\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.subplots_adjust(hspace=0.35, wspace=0.4)\n",
    "plt.savefig('Fig4.jpg', bbox_inches='tight', format='jpg', dpi=600)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07378d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
